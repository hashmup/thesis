\subsubsection{ハイブリッド並列化}
\label{subsubsec:hybrid}
\paragraph{OpenMP}~\\
NEURONでは, POSIX Threadによるスレッド並列が実装されているが, 京上ではOpenMPによるスレッド並列化しかサポートされていない.
宮本らによる先行研究\cite{miyamoto-master}において, OpenMPベースのバイブリッド並列化機構がNEURONに実装されていたため
本研究ではこのOpenMPが組み込まれているNEURONを用いてシミュレーションを行った.\\
上記の実装ではOMP\_NUM\_THREADSという環境変数の値に応じてスレッドの生成数を変えるものであったため, ジョブスクリプトの内部で
{\footnotesize
\begin{lstlisting}[caption=OpenMPスレッド数の指定,label=cluster-job-example,numbers=none]
export OMP_NUM_THREADS=16
\end{lstlisting}
}
とすることでOpenMPのスレッド数を指定することができる.\\

\paragraph{MPI}~\\
MPIはほとんどの並列計算機に入っているものであり, NEURONでも特別な変更を加えることなく利用することができる.\\
京とクラスタで指定する方法は違うものの, 双方ともにジョブスクリプトに利用したいプロセス数を記述することでMPIを使用することができる.\\
クラスタにおいては,
{\footnotesize
\begin{lstlisting}[caption=クラスタ MPIプロセス数の指定,label=cluster-mpi-num-process,numbers=none]
#PBS -l ppn=8
\end{lstlisting}
}
京においては,
{\footnotesize
\begin{lstlisting}[caption=京 MPIプロセス数の指定,label=k-mpi-num-process,numbers=none]
#PJM --mpi "proc=8"
\end{lstlisting}
}
とすることでMPIのプロセス数を指定することができる.\\

\paragraph{ハイブリッド並列化}~\\
並列化をする際にOpenMPとMPIと組み合わせることをOpenMPとMPIのハイブリッド並列化という.\\
OpenMPとMPIはそれぞれ長所と短所を持つが, 規模が小さい場合スレッド生成のコストが大きくMPIのみを利用するFlat MPIの性能が
ハイブリッド化するよりも優れていることが多い.\\
しかしながらノード数が増えていくに連れて, MPIプロセス間での通信に利用されるネットワーク通信部分がボトルネックとなっていく可能性が高くなっていく.
これは単一ノードの持つネットワーク通信に使えるリソースに対し通信対象となるMPIプロセスが増えすぎることが原因となる.\\
そのため計算規模が大きくなるほど, 同じノードの内部ではOpenMPを用いてCPUコア間で共有されているメモリを用いて通信を行い,
外部ノードとの通信にMPIを使うことでボトルネックとなる通信を分散させることができるハイブリッドの強みを生かすことができる.\\

（ TODO: 図）

また京のようにスレッドバリアと呼ばれるスレッドを高速に生成する機構を持っている計算機も存在するため,
OpenMPとMPIを用いる比率を実行マシンに依存するパラメータとして本研究では用い計算機にあった最適化を目指す.\\
